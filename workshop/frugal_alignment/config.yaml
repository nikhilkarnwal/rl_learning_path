# Model Configuration
model:
  name: "unsloth/llama-3-8b-bnb-4bit"  # Pre-quantized Llama-3-8B from Unsloth
  max_seq_length: 2048
  load_in_4bit: true

# LoRA Configuration
lora:
  r: 16  # LoRA rank
  alpha: 16  # LoRA alpha (scaling factor)
  dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Dataset Configuration
dataset:
  type: "huggingface"  # Options: "huggingface" or "local"
  name: "yahma/alpaca-cleaned"  # HuggingFace dataset name
  split: "train"
  # For local datasets:
  # type: "local"
  # path: "./data/training_data.json"

# Training Configuration
training:
  batch_size: 2
  gradient_accumulation_steps: 4  # Effective batch size = 2 * 4 = 8
  num_epochs: 3
  learning_rate: 2.0e-4
  warmup_steps: 10
  logging_steps: 10
  save_steps: 100
  save_total_limit: 3
  optimizer: "adamw_8bit"  # Memory-efficient optimizer
  weight_decay: 0.01
  lr_scheduler: "linear"
  seed: 42
  output_dir: "./outputs"
  save_merged_model: false  # Set to true to save merged 16-bit model
  use_wandb: true

# Weights & Biases Configuration
wandb:
  project: "llama3-finetuning"
  run_name: "llama3-8b-alignment"
