{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U transformers accelerate datasets evaluate sentencepiece protobuf pydantic rich\n",
    "\n",
    "import os, json, re, time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667488e",
   "metadata": {},
   "source": [
    "A1) Choose a small instruct model (3B–7B)\n",
    "\n",
    "Pick one that fits your Colab GPU. Good defaults:\n",
    "\n",
    "If you often get T4: start with 3B.\n",
    "\n",
    "If you have L4/A100 (Colab Pro): you can try 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # safe Colab choice\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # if you have stronger GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24cf738",
   "metadata": {},
   "source": [
    "A2) Load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bba6e",
   "metadata": {},
   "source": [
    "A3) Chat template + system prompts\n",
    "\n",
    "Most modern instruct models expect a “chat format”. We’ll use the tokenizer’s built-in chat template when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"Be concise. If unsure, say you don't know.\"\n",
    ")\n",
    "\n",
    "def build_chat_prompt(user_msg: str, system_msg: str = SYSTEM_PROMPT):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "    # apply_chat_template returns a string prompt; add_generation_prompt adds the assistant role start\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179154ec",
   "metadata": {},
   "source": [
    "A4) Basic generation function (greedy or sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text(user_msg, max_new_tokens=256, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    prompt = build_chat_prompt(user_msg)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else None,\n",
    "        top_p=top_p if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    # For chat templates, the decoded text includes prompt + response; extract response crudely\n",
    "    # (good enough for Week 1; we’ll improve later)\n",
    "    return out\n",
    "\n",
    "print(generate_text(\"Give me 5 birthday party themes for adults.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "\n",
    "class PartyIdeas(BaseModel):\n",
    "    themes: List[str]\n",
    "\n",
    "JSON_SYSTEM = (\n",
    "    \"You are a JSON-only assistant. \"\n",
    "    \"Return ONLY valid JSON. No markdown. No extra text.\"\n",
    ")\n",
    "\n",
    "def extract_first_json(text: str):\n",
    "    # simple heuristic: find first {...}\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "resp = generate_text(\n",
    "    \"Generate 5 adult birthday party themes. Output JSON with key 'themes' as a list of strings.\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    top_p=1.0,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "json_str = extract_first_json(resp)\n",
    "print(\"RAW:\\n\", resp)\n",
    "print(\"\\nJSON CANDIDATE:\\n\", json_str)\n",
    "\n",
    "try:\n",
    "    obj = PartyIdeas.model_validate_json(json_str)\n",
    "    print(\"\\nVALIDATED:\", obj)\n",
    "except ValidationError as e:\n",
    "    print(\"\\nVALIDATION ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66204c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "import threading\n",
    "\n",
    "def stream_generate(user_msg, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    prompt = build_chat_prompt(user_msg)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    kwargs = dict(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "\n",
    "    thread = threading.Thread(target=model.generate, kwargs=kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    out = \"\"\n",
    "    for chunk in streamer:\n",
    "        out += chunk\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "    return out\n",
    "\n",
    "_ = stream_generate(\"Explain RoPE in simple terms with a short example.\", max_new_tokens=200)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
