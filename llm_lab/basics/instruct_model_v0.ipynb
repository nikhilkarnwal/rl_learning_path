{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Download Qwen Instruct Model and Run through structured data   "
      ],
      "metadata": {
        "id": "f3lh9wMaDJEz"
      },
      "id": "f3lh9wMaDJEz"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6f3e3076",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3e3076",
        "outputId": "51f7667c-9f27-431b-9065-9fb6d24599db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/10.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m8.5/10.1 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/515.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os, json, re, time\n",
        "\n",
        "!pip -q install -U transformers accelerate datasets evaluate sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0667488e",
      "metadata": {
        "id": "0667488e"
      },
      "source": [
        "A1) Choose a small instruct model (3B–7B)\n",
        "\n",
        "Pick one that fits your Colab GPU. Good defaults:\n",
        "\n",
        "If you often get T4: start with 3B.\n",
        "\n",
        "If you have L4/A100 (Colab Pro): you can try 7B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7a90bbeb",
      "metadata": {
        "id": "7a90bbeb"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # safe Colab choice\n",
        "# MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # if you have stronger GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a24cf738",
      "metadata": {
        "id": "a24cf738"
      },
      "source": [
        "A2) Load model + tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6241d90e",
      "metadata": {
        "id": "6241d90e"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModel   ForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"Device:\", next(model.parameters()).device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XVwnT6RFAs6",
        "outputId": "e494db1f-76b9-45fe-f4ae-63b07ba9cd8b"
      },
      "id": "4XVwnT6RFAs6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen2ForCausalLM(\n",
            "  (model): Qwen2Model(\n",
            "    (embed_tokens): Embedding(151936, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-35): 36 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
            "    (rotary_emb): Qwen2RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9bba6e",
      "metadata": {
        "id": "9e9bba6e"
      },
      "source": [
        "A3) Chat template + system prompts\n",
        "\n",
        "Most modern instruct models expect a “chat format”. We’ll use the tokenizer’s built-in chat template when available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8630e8ac",
      "metadata": {
        "id": "8630e8ac"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful assistant. \"\n",
        "    \"Be concise. If unsure, say you don't know.\"\n",
        ")\n",
        "\n",
        "def build_chat_prompt(user_msg: str, system_msg: str = SYSTEM_PROMPT):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_msg},\n",
        "        {\"role\": \"user\", \"content\": user_msg},\n",
        "    ]\n",
        "    # apply_chat_template returns a string prompt; add_generation_prompt adds the assistant role start\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179154ec",
      "metadata": {
        "id": "179154ec"
      },
      "source": [
        "A4) Basic generation function (greedy or sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "230c086e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "230c086e",
        "outputId": "8a1e1a4a-4e65-4ac5-90e2-9fe56f722930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT:\n",
            " <|im_start|>system\n",
            "You are a helpful assistant. Be concise. If unsure, say you don't know.<|im_end|>\n",
            "<|im_start|>user\n",
            "Give me 5 ideas to celebrate 3rd marraige aniversary in las vegas.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "system\n",
            "You are a helpful assistant. Be concise. If unsure, say you don't know.\n",
            "user\n",
            "Give me 5 ideas to celebrate 3rd marraige aniversary in las vegas.\n",
            "assistant\n",
            "Here are 5 ideas for celebrating a 3rd marriage anniversary in Las Vegas:\n",
            "\n",
            "1. Enjoy a sunset cruise on the Las Vegas Strip.\n",
            "2. Visit the Bellagio fountains and have dinner at their Viceroy restaurant.\n",
            "3. Attend a show at the Planet Hollywood Theater.\n",
            "4. Have a private dinner at one of Las Vegas' luxury hotels.\n",
            "5. Take a helicopter tour over the city.\n"
          ]
        }
      ],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_text(user_msg, max_new_tokens=256, temperature=0.7, top_p=0.9, do_sample=True):\n",
        "    prompt = build_chat_prompt(user_msg)\n",
        "    print(\"PROMPT:\\n\", prompt)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    gen = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature if do_sample else None,\n",
        "        top_p=top_p if do_sample else None,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "    # For chat templates, the decoded text includes prompt + response; extract response crudely\n",
        "    # (good enough for Week 1; we’ll improve later)\n",
        "    return out\n",
        "\n",
        "print(generate_text(\"Give me 5 ideas to celebrate 3rd marraige aniversary in las vegas.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cfeda04",
        "outputId": "e47815e1-3a05-4ae4-c500-bd36d44eef56"
      },
      "source": [
        "prompt_example = build_chat_prompt(\"What is the capital of France?\")\n",
        "inputs_example = tokenizer(prompt_example, return_tensors=\"pt\")\n",
        "print(inputs_example)"
      ],
      "id": "0cfeda04",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
            "           2823,  63594,     13,   1416,  42903,     11,   1977,    498,   1513,\n",
            "            944,   1414,     13, 151645,    198, 151644,    872,    198,   3838,\n",
            "            374,    279,   6722,    315,   9625,     30, 151645,    198, 151644,\n",
            "          77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f7cd66fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7cd66fa",
        "outputId": "d62e527a-a2d8-42ef-fc4a-65b10611c138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW:\n",
            " system\n",
            "You are a helpful assistant. Be concise. If unsure, say you don't know.\n",
            "user\n",
            "Generate 5 adult birthday party themes. Output JSON with key 'themes' as a list of strings.\n",
            "assistant\n",
            "```json\n",
            "{\n",
            "  \"themes\": [\n",
            "    \"Vintage Glamour\",\n",
            "    \"Under the Sea\",\n",
            "    \"Rock & Roll Night\",\n",
            "    \"Garden Party\",\n",
            "    \"Space Adventure\"\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "JSON CANDIDATE:\n",
            " {\n",
            "  \"themes\": [\n",
            "    \"Vintage Glamour\",\n",
            "    \"Under the Sea\",\n",
            "    \"Rock & Roll Night\",\n",
            "    \"Garden Party\",\n",
            "    \"Space Adventure\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "VALIDATED: themes=['Vintage Glamour', 'Under the Sea', 'Rock & Roll Night', 'Garden Party', 'Space Adventure']\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, ValidationError\n",
        "from typing import List\n",
        "\n",
        "class PartyIdeas(BaseModel):\n",
        "    themes: List[str]\n",
        "\n",
        "JSON_SYSTEM = (\n",
        "    \"You are a JSON-only assistant. \"\n",
        "    \"Return ONLY valid JSON. No markdown. No extra text.\"\n",
        ")\n",
        "\n",
        "def extract_first_json(text: str):\n",
        "    # simple heuristic: find first {...}\n",
        "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
        "    return m.group(0) if m else None\n",
        "\n",
        "resp = generate_text(\n",
        "    \"Generate 5 adult birthday party themes. Output JSON with key 'themes' as a list of strings.\",\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.2,\n",
        "    top_p=1.0,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "json_str = extract_first_json(resp)\n",
        "print(\"RAW:\\n\", resp)\n",
        "print(\"\\nJSON CANDIDATE:\\n\", json_str)\n",
        "\n",
        "try:\n",
        "    obj = PartyIdeas.model_validate_json(json_str)\n",
        "    print(\"\\nVALIDATED:\", obj)\n",
        "except ValidationError as e:\n",
        "    print(\"\\nVALIDATION ERROR:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "66204c05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66204c05",
        "outputId": "42cf90b3-b879-4062-da7e-9dd895784328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoPE is a technique used in transformer models to handle sequences that have different lengths by incorporating positional information into the model's input embeddings. This helps in preserving the relative position between tokens, which is crucial for tasks like language modeling.\n",
            "\n",
            "In simple terms, imagine you have two sentences: \"I love cats\" and \"Cats love me\". These sentences are the same but with different orders of words. RoPE allows the model to understand that \"love\" and \"cats\" are closer together in the first sentence than they are in the second one.\n",
            "\n",
            "A short example:\n",
            "- Sentence A: \"Hello world\"\n",
            "- Sentence B: \"world Hello\"\n",
            "\n",
            "Without RoPE, a transformer might treat \"Hello\" and \"world\" as equally distant. With RoPE, it understands that \"Hello\" is closer to \"world\" in the first sentence compared to the second sentence.\n",
            "\n",
            "RoPE achieves this by adding a special token at the beginning of each sequence that encodes the relative length of the sequence and\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "import threading\n",
        "\n",
        "def stream_generate(user_msg, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
        "    prompt = build_chat_prompt(user_msg)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    kwargs = dict(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "\n",
        "    thread = threading.Thread(target=model.generate, kwargs=kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    out = \"\"\n",
        "    for chunk in streamer:\n",
        "        out += chunk\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "    print()\n",
        "    return out\n",
        "\n",
        "_ = stream_generate(\"Explain RoPE in simple terms with a short example.\", max_new_tokens=200)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}