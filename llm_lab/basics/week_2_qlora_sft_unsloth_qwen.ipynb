{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installation commands"
      ],
      "metadata": {
        "id": "qOPA9-OE8uPC"
      },
      "id": "qOPA9-OE8uPC"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth  # Do this in local & cloud setups\n",
        "else:\n",
        "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
        "    xformers = 'xformers==' + {'2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.33.post1\")\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
        "!pip install transformers==4.56.2 && pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "Vd4wG9zmBHXs"
      },
      "id": "Vd4wG9zmBHXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "86d40334",
      "metadata": {
        "id": "86d40334"
      },
      "source": [
        "Load model in 4-bit + prep LoRA (Unsloth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0354918a",
      "metadata": {
        "id": "0354918a"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "max_seq_length = 2048   # 1024 if you hit OOM\n",
        "dtype = None            # Unsloth picks best (bf16 if supported)\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# LoRA config — good defaults for 3B\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,                         # 8–16 sweet spot\n",
        "    target_modules = [\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",  # memory saver\n",
        "    random_state = 3407,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "LuLXb_EJDxEn"
      },
      "id": "LuLXb_EJDxEn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"You are a helpful assistant. Be concise. If unsure, say you don't know.\"\n",
        "\n",
        "def format_example(user, assistant, system=SYSTEM_PROMPT):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": user},\n",
        "        {\"role\": \"assistant\", \"content\": assistant},\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "SnuzzCxLFmn_"
      },
      "id": "SnuzzCxLFmn_",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = []\n",
        "\n",
        "# 1) JSON extraction tasks\n",
        "json_examples = [\n",
        "    (\"Extract into JSON with keys name and date (YYYY-MM-DD): Alice Johnson, born Jan 5 1994\",\n",
        "     '{\"name\":\"Alice Johnson\",\"date\":\"1994-01-05\"}'),\n",
        "    (\"Extract into JSON with keys company and amount_usd: 'Acme Corp raised $12M'\",\n",
        "     '{\"company\":\"Acme Corp\",\"amount_usd\":12000000}'),\n",
        "]\n",
        "\n",
        "# 2) Bullet summaries\n",
        "sum_examples = [\n",
        "    (\"Summarize in 3 bullets: Q4 revenue grew 20% QoQ, churn dropped to 2.1%, onboarding time reduced by 35%.\",\n",
        "     \"- Revenue grew 20% QoQ in Q4\\n- Churn decreased to 2.1%\\n- Onboarding time improved by 35%\"),\n",
        "]\n",
        "\n",
        "# 3) Uncertainty / refusal\n",
        "refusal_examples = [\n",
        "    (\"What is the CEO of CompanyX as of today? (CompanyX is not specified.)\",\n",
        "     \"I don't know. You didn’t specify which company you mean, so I can’t answer accurately.\"),\n",
        "    (\"Give me the exact birthday of an unknown person named John.\",\n",
        "     \"I don't know. I don’t have enough information to determine John’s birthday.\"),\n",
        "]\n",
        "\n",
        "for u,a in json_examples + sum_examples + refusal_examples:\n",
        "    train_pairs.append((u,a))\n",
        "\n",
        "# Expand a bit by paraphrasing prompts lightly (simple augmentation)\n",
        "augmented = []\n",
        "for (u,a) in train_pairs:\n",
        "    for prefix in [\"Please\", \"Kindly\", \"\"]:\n",
        "        uu = (prefix + \" \" + u).strip()\n",
        "        augmented.append((uu,a))\n",
        "train_pairs = augmented\n",
        "\n",
        "print(\"Train examples:\", len(train_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlmmiJl5KskZ",
        "outputId": "2f026bdd-95dc-4ce7-bb60-7c8b3c928f60"
      },
      "id": "hlmmiJl5KskZ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train examples: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_texts = [format_example(u,a) for (u,a) in train_pairs]\n",
        "train_ds = Dataset.from_dict({\"text\": train_texts})\n",
        "train_ds[0][\"text\"][:400]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "OiTTlDBHK4b2",
        "outputId": "57f17de8-78a1-4740-9792-39d31812cb03"
      },
      "id": "OiTTlDBHK4b2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are a helpful assistant. Be concise. If unsure, say you don\\'t know.<|im_end|>\\n<|im_start|>user\\nPlease Extract into JSON with keys name and date (YYYY-MM-DD): Alice Johnson, born Jan 5 1994<|im_end|>\\n<|im_start|>assistant\\n{\"name\":\"Alice Johnson\",\"date\":\"1994-01-05\"}<|im_end|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_ds,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,  # effective batch = 16\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,                  # Week2 quick run; increase to 200–800 later\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 5,\n",
        "        output_dir = \"outputs\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609,
          "referenced_widgets": [
            "2ffadb012f574e32970e22dd5b6357e3",
            "57cf59b7072243d182d9ab5fa4a725f7",
            "f51b5276a4374667b82fcbc5e14f69b3",
            "8ccdf145b9fe4c7db0383c54f3b90035",
            "8dcd80299dcf41c5a71774c4419d1aa0",
            "791782df487348fdbad948d8f0070d2e",
            "bed1ce412fe744c7869ae6436f0d7ca5",
            "392a88e649024ab5a8c966f0cdb3dd6c",
            "587a4a83c70e4d0eae869f2724ece9e2",
            "750c7f50606b4df2b1e9924179e47396",
            "8431ee27337947928db8dc8851a76000"
          ]
        },
        "id": "kIsbEH8YLXR2",
        "outputId": "2386a558-8dc7-494f-b8ee-7edb3cb7dcaa"
      },
      "id": "kIsbEH8YLXR2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/15 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ffadb012f574e32970e22dd5b6357e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 15 | Num Epochs = 60 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 29,933,568 of 3,115,872,256 (0.96% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:13, Epoch 60/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.336700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.144200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.043300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.036500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.035500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.035300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=0.47642183924714726, metrics={'train_runtime': 223.9851, 'train_samples_per_second': 4.286, 'train_steps_per_second': 0.268, 'total_flos': 1295774785536000.0, 'train_loss': 0.47642183924714726, 'epoch': 60.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def chat(user_msg, system=SYSTEM_PROMPT, max_new_tokens=256, temperature=0.3, top_p=0.9):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":system},\n",
        "        {\"role\":\"user\",\"content\":user_msg},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(chat(\"Extract into JSON with keys name and date: Bob Smith, born Feb 3 2001\",\n",
        "           system=\"You are a JSON-only assistant. Return only valid JSON.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E95iFWdLzl3",
        "outputId": "8e51e51e-3863-480d-cf97-34933d29c8aa"
      },
      "id": "6E95iFWdLzl3",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "You are a JSON-only assistant. Return only valid JSON.\n",
            "user\n",
            "Extract into JSON with keys name and date: Bob Smith, born Feb 3 2001\n",
            "assistant\n",
            "{\"name\":\"Bob Smith\",\"date\":\"2001-02-03\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat(\"Extract into JSON with keys name and date: Bob Smith, born 3rd Feb  2026\",\n",
        "           system=\"You are a JSON-only assistant. Return only valid JSON.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG2bIcl8Nd3j",
        "outputId": "080a3a0c-218a-4c37-fdb4-e60c6b96b865"
      },
      "id": "OG2bIcl8Nd3j",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "You are a JSON-only assistant. Return only valid JSON.\n",
            "user\n",
            "Extract into JSON with keys name and date: Bob Smith, born 3rd Feb  2026\n",
            "assistant\n",
            "{\"name\":\"Bob Smith\",\"date\":\"3rd Feb  2026\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval"
      ],
      "metadata": {
        "id": "oZSDOpiDOX0I"
      },
      "id": "oZSDOpiDOX0I"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_items = [\n",
        "    {\n",
        "        \"id\":\"json_1\",\n",
        "        \"prompt\":\"Extract into JSON with keys name and date (YYYY-MM-DD): Alice Johnson, born Jan 5 1994\",\n",
        "        \"checks\":[\"json_valid\",\"json_has_keys:name,date\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\":\"sum_1\",\n",
        "        \"prompt\":\"Summarize in 3 bullets: Our Q4 revenue grew 20% QoQ, churn dropped to 2.1%, onboarding time reduced by 35%.\",\n",
        "        \"checks\":[\"bullet_count:3\",\"max_words:60\"]\n",
        "    },\n",
        "    {\n",
        "        \"id\":\"refuse_1\",\n",
        "        \"prompt\":\"What is the CEO of CompanyX as of today? (CompanyX is not specified.)\",\n",
        "        \"checks\":[\"contains_idk\",\"max_words:40\"]\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "K7-yBF0fNtF3"
      },
      "id": "K7-yBF0fNtF3",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def extract_first_json(text: str):\n",
        "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
        "    return m.group(0) if m else None\n",
        "\n",
        "def check_json_valid(text: str) -> bool:\n",
        "    s = extract_first_json(text)\n",
        "    if not s: return False\n",
        "    try:\n",
        "        json.loads(s)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def check_json_has_keys(text: str, keys):\n",
        "    s = extract_first_json(text)\n",
        "    if not s: return False\n",
        "    try:\n",
        "        obj = json.loads(s)\n",
        "    except:\n",
        "        return False\n",
        "    return all(k in obj for k in keys)\n",
        "\n",
        "def check_bullet_count(text: str, n: int) -> bool:\n",
        "    bullets = [ln.strip() for ln in text.splitlines() if ln.strip().startswith((\"-\", \"•\", \"*\"))]\n",
        "    return len(bullets) == n\n",
        "\n",
        "def check_max_words(text: str, n: int) -> bool:\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    return len(words) <= n\n",
        "\n",
        "def check_contains_idk(text: str) -> bool:\n",
        "    t = text.lower()\n",
        "    return (\"i don't know\" in t) or (\"i do not know\" in t) or (\"not enough information\" in t) or (\"cannot\" in t)\n",
        "\n",
        "def run_checks(output: str, checks):\n",
        "    results = {}\n",
        "    for chk in checks:\n",
        "        if chk == \"json_valid\":\n",
        "            results[chk] = check_json_valid(output)\n",
        "        elif chk.startswith(\"json_has_keys:\"):\n",
        "            keys = chk.split(\":\",1)[1].split(\",\")\n",
        "            results[chk] = check_json_has_keys(output, keys)\n",
        "        elif chk.startswith(\"bullet_count:\"):\n",
        "            n = int(chk.split(\":\")[1])\n",
        "            results[chk] = check_bullet_count(output, n)\n",
        "        elif chk.startswith(\"max_words:\"):\n",
        "            n = int(chk.split(\":\")[1])\n",
        "            results[chk] = check_max_words(output, n)\n",
        "        elif chk == \"contains_idk\":\n",
        "            results[chk] = check_contains_idk(output)\n",
        "        else:\n",
        "            results[chk] = False\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "hVLOSiHkOaXE"
      },
      "id": "hVLOSiHkOaXE",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_suite(items, system=SYSTEM_PROMPT, temperature=0.3, top_p=0.9):\n",
        "    rows = []\n",
        "    for it in items:\n",
        "        out = chat(it[\"prompt\"], system=system, max_new_tokens=200, temperature=temperature, top_p=top_p)\n",
        "        checks = run_checks(out, it[\"checks\"])\n",
        "        score = sum(checks.values()) / max(1, len(checks))\n",
        "        rows.append({\n",
        "            \"id\": it[\"id\"],\n",
        "            \"prompt\": it[\"prompt\"],\n",
        "            \"score\": score,\n",
        "            \"checks\": checks,\n",
        "            \"output_preview\": out[-400:],\n",
        "        })\n",
        "        if score < 1.0:\n",
        "          print(rows[-1])\n",
        "    avg = sum(r[\"score\"] for r in rows) / len(rows)\n",
        "    return avg, rows\n",
        "\n",
        "avg, rows = eval_suite(eval_items, system=SYSTEM_PROMPT, temperature=0.2, top_p=1.0)\n",
        "avg, rows[0][\"checks\"], rows[0][\"output_preview\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJA56cHkPKhe",
        "outputId": "93a468ee-d5b3-4295-8c45-bee3a6a9163f"
      },
      "id": "nJA56cHkPKhe",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'refuse_1', 'prompt': 'What is the CEO of CompanyX as of today? (CompanyX is not specified.)', 'score': 0.5, 'checks': {'contains_idk': True, 'max_words:40': False}, 'output_preview': \"system\\nYou are a helpful assistant. Be concise. If unsure, say you don't know.\\nuser\\nWhat is the CEO of CompanyX as of today? (CompanyX is not specified.)\\nassistant\\nI don't know. You didn’t specify which company you mean, so I can’t answer accurately.\"}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8333333333333334,\n",
              " {'json_valid': True, 'json_has_keys:name,date': True},\n",
              " 'system\\nYou are a helpful assistant. Be concise. If unsure, say you don\\'t know.\\nuser\\nExtract into JSON with keys name and date (YYYY-MM-DD): Alice Johnson, born Jan 5 1994\\nassistant\\n{\"name\":\"Alice Johnson\",\"date\":\"1994-01-05\"}')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"qwen25_3b_week2_lora\"\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "print(\"Saved to:\", SAVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqnD-QPnPYF3",
        "outputId": "bc18ac05-332c-49d2-f534-952bde423b8f"
      },
      "id": "hqnD-QPnPYF3",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: qwen25_3b_week2_lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIqc64GpR5Ij"
      },
      "id": "iIqc64GpR5Ij",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ffadb012f574e32970e22dd5b6357e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57cf59b7072243d182d9ab5fa4a725f7",
              "IPY_MODEL_f51b5276a4374667b82fcbc5e14f69b3",
              "IPY_MODEL_8ccdf145b9fe4c7db0383c54f3b90035"
            ],
            "layout": "IPY_MODEL_8dcd80299dcf41c5a71774c4419d1aa0"
          }
        },
        "57cf59b7072243d182d9ab5fa4a725f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_791782df487348fdbad948d8f0070d2e",
            "placeholder": "​",
            "style": "IPY_MODEL_bed1ce412fe744c7869ae6436f0d7ca5",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=4): 100%"
          }
        },
        "f51b5276a4374667b82fcbc5e14f69b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392a88e649024ab5a8c966f0cdb3dd6c",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_587a4a83c70e4d0eae869f2724ece9e2",
            "value": 15
          }
        },
        "8ccdf145b9fe4c7db0383c54f3b90035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_750c7f50606b4df2b1e9924179e47396",
            "placeholder": "​",
            "style": "IPY_MODEL_8431ee27337947928db8dc8851a76000",
            "value": " 15/15 [00:07&lt;00:00,  2.45 examples/s]"
          }
        },
        "8dcd80299dcf41c5a71774c4419d1aa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "791782df487348fdbad948d8f0070d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed1ce412fe744c7869ae6436f0d7ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "392a88e649024ab5a8c966f0cdb3dd6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "587a4a83c70e4d0eae869f2724ece9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "750c7f50606b4df2b1e9924179e47396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8431ee27337947928db8dc8851a76000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}